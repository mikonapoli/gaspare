[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "gaspare",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.",
    "crumbs": [
      "gaspare"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "gaspare",
    "section": "Developer Guide",
    "text": "Developer Guide\nIf you are new to using nbdev here are some useful pointers to get you started.\n\nInstall gaspare in Development mode\n# make sure gaspare package is installed in development mode\n$ pip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to gaspare\n$ nbdev_prepare",
    "crumbs": [
      "gaspare"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "gaspare",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/mikonapoli/gaspare.git\nor from conda\n$ conda install -c mikonapoli gaspare\nor from pypi\n$ pip install gaspare\n\n\nDocumentation\nDocumentation can be found hosted on this GitHub repository’s pages. Additionally you can find package manager specific guidelines on conda and pypi respectively.",
    "crumbs": [
      "gaspare"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "gaspare",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2",
    "crumbs": [
      "gaspare"
    ]
  },
  {
    "objectID": "00_core.html",
    "href": "00_core.html",
    "title": "gaspare",
    "section": "",
    "text": "Exported source\nimport os\nimport base64\n\nimport PIL\nimport mimetypes\nimport inspect\n\nfrom typing import Union\nfrom functools import wraps\nfrom google import genai\nfrom google.genai import types\n\nfrom fastcore.all import *\nfrom fastcore.docments import *\n\n\n\n\nExported source\nall_model_types = {\n    \"gemini-2.0-flash\": \"llm-vertex#gemini-2.0-flash\",\n    \"gemini-2.0-flash-001\": \"llm-vertex#gemini-2.0-flash\",\n    \"gemini-2.0-pro-exp-02-05\": \"llm#gemini-2.0-pro\",\n    \"gemini-2.0-flash-lite-preview-02-05\": \"llm#gemini-2.0-flash-lite\",\n    \"gemini-1.5-flash\": \"llm-vertex#gemini-1.5-flash\",\n    \"gemini-1.5-pro\": \"llm-vertex#gemini-1.5-pro\",\n    \"gemini-1.5-pro-002\": \"llm-vertex#gemini-1.5-pro\",\n    \"gemini-1.5-flash-8b\": \"llm#gemini-1.5-flash-8b\",\n    \"gemini-2.0-flash-thinking-exp-01-21\": \"llm-thinking#gemini-2.0-flash-thinking\",\n    \"imagen-3.0-generate-002\": \"imagen#imagen-3.0\"\n}\n\nthinking_models = [m for m in all_model_types if \"thinking\" in all_model_types[m]]\n\nimagen_models = [m for m in all_model_types if \"imagen\" in all_model_types[m]]\n\nvertex_models = [m for m in all_model_types if \"vertex\" in all_model_types[m]]\n\nmodels = [m for m in all_model_types if \"llm\" in all_model_types[m]]\n\nmodels\n\n\n['gemini-2.0-flash',\n 'gemini-2.0-flash-001',\n 'gemini-2.0-pro-exp-02-05',\n 'gemini-2.0-flash-lite-preview-02-05',\n 'gemini-1.5-flash',\n 'gemini-1.5-pro',\n 'gemini-1.5-pro-002',\n 'gemini-1.5-flash-8b',\n 'gemini-2.0-flash-thinking-exp-01-21']\n\n\nGemini has several types of models and only some of those work with VertexAI client. Input wise, essentially all models support images and text, and since Gemini 1.5 all (except gemini-2.0-flash-thinking-exp-01-21) support audio and video input.\nRight now only imagen models support non text output, but image and audio outputs are coming to Gemini 2 “soon”.\nMain models support function calling, structured output, code executions (with a few exception on the preview models) and all models support streaming and system prompts.\nAlthough it would be nice to provide a uniform library interface with cosette and claudette, it’s probably a bit too complex for the moment.\n\nfrom IPython.display import Markdown\n\n\nGEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\", None)\nMarkdown(\"* \" + \"* \".join([f\"`{model.name}`:  {model.description}\\n\" for model in genai.Client(api_key=GEMINI_API_KEY).models.list()]))\n\n\nmodels/chat-bison-001: A legacy text-only model optimized for chat conversations\nmodels/text-bison-001: A legacy model that understands text and generates text as an output\nmodels/embedding-gecko-001: Obtain a distributed representation of a text.\nmodels/gemini-1.0-pro-latest: The original Gemini 1.0 Pro model. This model will be discontinued on February 15th, 2025. Move to a newer Gemini version.\nmodels/gemini-1.0-pro: The best model for scaling across a wide range of tasks\nmodels/gemini-pro: The best model for scaling across a wide range of tasks\nmodels/gemini-1.0-pro-001: The original Gemini 1.0 Pro model version that supports tuning. Gemini 1.0 Pro will be discontinued on February 15th, 2025. Move to a newer Gemini version.\nmodels/gemini-1.0-pro-vision-latest: The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\nmodels/gemini-pro-vision: The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\nmodels/gemini-1.5-pro-latest: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.\nmodels/gemini-1.5-pro-001: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\nmodels/gemini-1.5-pro-002: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in September of 2024.\nmodels/gemini-1.5-pro: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\nmodels/gemini-1.5-flash-latest: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\nmodels/gemini-1.5-flash-001: Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\nmodels/gemini-1.5-flash-001-tuning: Version of Gemini 1.5 Flash that supports tuning, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\nmodels/gemini-1.5-flash: Alias that points to the most recent stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\nmodels/gemini-1.5-flash-002: Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in September of 2024.\nmodels/gemini-1.5-flash-8b: Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\nmodels/gemini-1.5-flash-8b-001: Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\nmodels/gemini-1.5-flash-8b-latest: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\nmodels/gemini-1.5-flash-8b-exp-0827: Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\nmodels/gemini-1.5-flash-8b-exp-0924: Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\nmodels/gemini-2.0-flash-exp: Gemini 2.0 Flash Experimental\nmodels/gemini-2.0-flash: Gemini 2.0 Flash\nmodels/gemini-2.0-flash-001: Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.\nmodels/gemini-2.0-flash-lite-preview: Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\nmodels/gemini-2.0-flash-lite-preview-02-05: Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\nmodels/gemini-2.0-pro-exp: Experimental release (February 5th, 2025) of Gemini 2.0 Pro\nmodels/gemini-2.0-pro-exp-02-05: Experimental release (February 5th, 2025) of Gemini 2.0 Pro\nmodels/gemini-exp-1206: Experimental release (February 5th, 2025) of Gemini 2.0 Pro\nmodels/gemini-2.0-flash-thinking-exp-01-21: Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking\nmodels/gemini-2.0-flash-thinking-exp: Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking\nmodels/gemini-2.0-flash-thinking-exp-1219: Gemini 2.0 Flash Thinking Experimental\nmodels/learnlm-1.5-pro-experimental: Alias that points to the most recent stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.\nmodels/embedding-001: Obtain a distributed representation of a text.\nmodels/text-embedding-004: Obtain a distributed representation of a text.\nmodels/aqa: Model trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.\nmodels/imagen-3.0-generate-002: Vertex served Imagen 3.0 002 model\n\n\n\nThe Genai API exposes way more models than what we have included, but most of those are outdated, the list is not properly maintained and not all models behave in the same way. Although all of them will still be available, it’s is fine to just restrict to a few select ones.",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "00_core.html#setup",
    "href": "00_core.html#setup",
    "title": "gaspare",
    "section": "",
    "text": "Exported source\nimport os\nimport base64\n\nimport PIL\nimport mimetypes\nimport inspect\n\nfrom typing import Union\nfrom functools import wraps\nfrom google import genai\nfrom google.genai import types\n\nfrom fastcore.all import *\nfrom fastcore.docments import *\n\n\n\n\nExported source\nall_model_types = {\n    \"gemini-2.0-flash\": \"llm-vertex#gemini-2.0-flash\",\n    \"gemini-2.0-flash-001\": \"llm-vertex#gemini-2.0-flash\",\n    \"gemini-2.0-pro-exp-02-05\": \"llm#gemini-2.0-pro\",\n    \"gemini-2.0-flash-lite-preview-02-05\": \"llm#gemini-2.0-flash-lite\",\n    \"gemini-1.5-flash\": \"llm-vertex#gemini-1.5-flash\",\n    \"gemini-1.5-pro\": \"llm-vertex#gemini-1.5-pro\",\n    \"gemini-1.5-pro-002\": \"llm-vertex#gemini-1.5-pro\",\n    \"gemini-1.5-flash-8b\": \"llm#gemini-1.5-flash-8b\",\n    \"gemini-2.0-flash-thinking-exp-01-21\": \"llm-thinking#gemini-2.0-flash-thinking\",\n    \"imagen-3.0-generate-002\": \"imagen#imagen-3.0\"\n}\n\nthinking_models = [m for m in all_model_types if \"thinking\" in all_model_types[m]]\n\nimagen_models = [m for m in all_model_types if \"imagen\" in all_model_types[m]]\n\nvertex_models = [m for m in all_model_types if \"vertex\" in all_model_types[m]]\n\nmodels = [m for m in all_model_types if \"llm\" in all_model_types[m]]\n\nmodels\n\n\n['gemini-2.0-flash',\n 'gemini-2.0-flash-001',\n 'gemini-2.0-pro-exp-02-05',\n 'gemini-2.0-flash-lite-preview-02-05',\n 'gemini-1.5-flash',\n 'gemini-1.5-pro',\n 'gemini-1.5-pro-002',\n 'gemini-1.5-flash-8b',\n 'gemini-2.0-flash-thinking-exp-01-21']\n\n\nGemini has several types of models and only some of those work with VertexAI client. Input wise, essentially all models support images and text, and since Gemini 1.5 all (except gemini-2.0-flash-thinking-exp-01-21) support audio and video input.\nRight now only imagen models support non text output, but image and audio outputs are coming to Gemini 2 “soon”.\nMain models support function calling, structured output, code executions (with a few exception on the preview models) and all models support streaming and system prompts.\nAlthough it would be nice to provide a uniform library interface with cosette and claudette, it’s probably a bit too complex for the moment.\n\nfrom IPython.display import Markdown\n\n\nGEMINI_API_KEY = os.environ.get(\"GEMINI_API_KEY\", None)\nMarkdown(\"* \" + \"* \".join([f\"`{model.name}`:  {model.description}\\n\" for model in genai.Client(api_key=GEMINI_API_KEY).models.list()]))\n\n\nmodels/chat-bison-001: A legacy text-only model optimized for chat conversations\nmodels/text-bison-001: A legacy model that understands text and generates text as an output\nmodels/embedding-gecko-001: Obtain a distributed representation of a text.\nmodels/gemini-1.0-pro-latest: The original Gemini 1.0 Pro model. This model will be discontinued on February 15th, 2025. Move to a newer Gemini version.\nmodels/gemini-1.0-pro: The best model for scaling across a wide range of tasks\nmodels/gemini-pro: The best model for scaling across a wide range of tasks\nmodels/gemini-1.0-pro-001: The original Gemini 1.0 Pro model version that supports tuning. Gemini 1.0 Pro will be discontinued on February 15th, 2025. Move to a newer Gemini version.\nmodels/gemini-1.0-pro-vision-latest: The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\nmodels/gemini-pro-vision: The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\nmodels/gemini-1.5-pro-latest: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.\nmodels/gemini-1.5-pro-001: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\nmodels/gemini-1.5-pro-002: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in September of 2024.\nmodels/gemini-1.5-pro: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\nmodels/gemini-1.5-flash-latest: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\nmodels/gemini-1.5-flash-001: Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\nmodels/gemini-1.5-flash-001-tuning: Version of Gemini 1.5 Flash that supports tuning, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\nmodels/gemini-1.5-flash: Alias that points to the most recent stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\nmodels/gemini-1.5-flash-002: Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in September of 2024.\nmodels/gemini-1.5-flash-8b: Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\nmodels/gemini-1.5-flash-8b-001: Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\nmodels/gemini-1.5-flash-8b-latest: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\nmodels/gemini-1.5-flash-8b-exp-0827: Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\nmodels/gemini-1.5-flash-8b-exp-0924: Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\nmodels/gemini-2.0-flash-exp: Gemini 2.0 Flash Experimental\nmodels/gemini-2.0-flash: Gemini 2.0 Flash\nmodels/gemini-2.0-flash-001: Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.\nmodels/gemini-2.0-flash-lite-preview: Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\nmodels/gemini-2.0-flash-lite-preview-02-05: Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\nmodels/gemini-2.0-pro-exp: Experimental release (February 5th, 2025) of Gemini 2.0 Pro\nmodels/gemini-2.0-pro-exp-02-05: Experimental release (February 5th, 2025) of Gemini 2.0 Pro\nmodels/gemini-exp-1206: Experimental release (February 5th, 2025) of Gemini 2.0 Pro\nmodels/gemini-2.0-flash-thinking-exp-01-21: Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking\nmodels/gemini-2.0-flash-thinking-exp: Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking\nmodels/gemini-2.0-flash-thinking-exp-1219: Gemini 2.0 Flash Thinking Experimental\nmodels/learnlm-1.5-pro-experimental: Alias that points to the most recent stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.\nmodels/embedding-001: Obtain a distributed representation of a text.\nmodels/text-embedding-004: Obtain a distributed representation of a text.\nmodels/aqa: Model trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.\nmodels/imagen-3.0-generate-002: Vertex served Imagen 3.0 002 model\n\n\n\nThe Genai API exposes way more models than what we have included, but most of those are outdated, the list is not properly maintained and not all models behave in the same way. Although all of them will still be available, it’s is fine to just restrict to a few select ones.",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "00_core.html#genai-sdk",
    "href": "00_core.html#genai-sdk",
    "title": "gaspare",
    "section": "Genai SDK",
    "text": "Genai SDK\n\nc = genai.Client(api_key=GEMINI_API_KEY)\n\nThis how the Gemini SDK gives access to the API. The client itself, in particular has a number of subclients/methods that give access to the different endpoint and functionalities. The main ones we are interested in are models, that is the main interface with all the models, and chat which essentially wraps the former with a few convenience functionalities for handling message history.\n\nmodel = models[0]\nmodel\n\n'gemini-2.0-flash'\n\n\n\nr = c.models.generate_content(model=model, contents=\"Hi Gemini! Are you ready to work?\")\nr\n\nGenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text='Yes, I am ready to work! What can I do for you?\\n')], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=-0.36470311880111694, finish_reason=&lt;FinishReason.STOP: 'STOP'&gt;, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], model_version='gemini-2.0-flash', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=16, prompt_token_count=9, total_token_count=25), automatic_function_calling_history=[], parsed=None)\n\n\n\nr.to_json_dict()\n\n{'candidates': [{'content': {'parts': [{'text': 'Yes, I am ready to work! What can I do for you?\\n'}],\n    'role': 'model'},\n   'avg_logprobs': -0.36470311880111694,\n   'finish_reason': 'STOP'}],\n 'model_version': 'gemini-2.0-flash',\n 'usage_metadata': {'candidates_token_count': 16,\n  'prompt_token_count': 9,\n  'total_token_count': 25},\n 'automatic_function_calling_history': []}\n\n\n\nprint(r.text)\n\nYes, I am ready to work! What can I do for you?\n\n\n\nIn typical Google fashion (they really like their protobufs), the response is a nested mess of pydantic models. Luckily they all have a few convenience methods to make everything a bit more accessible.\n\nhelp(genai._common.BaseModel)\n\nHelp on class BaseModel in module google.genai._common:\n\nclass BaseModel(pydantic.main.BaseModel)\n |  BaseModel() -&gt; None\n |\n |  Method resolution order:\n |      BaseModel\n |      pydantic.main.BaseModel\n |      builtins.object\n |\n |  Methods defined here:\n |\n |  to_json_dict(self) -&gt; dict[str, object]\n |\n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |\n |  __weakref__\n |      list of weak references to the object\n |\n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |\n |  __abstractmethods__ = frozenset()\n |\n |  __annotations__ = {}\n |\n |  __class_vars__ = set()\n |\n |  __private_attributes__ = {}\n |\n |  __pydantic_complete__ = True\n |\n |  __pydantic_computed_fields__ = {}\n |\n |  __pydantic_core_schema__ = {'cls': &lt;class 'google.genai._common.BaseMo...\n |\n |  __pydantic_custom_init__ = False\n |\n |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n |\n |  __pydantic_fields__ = {}\n |\n |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n |\n |  __pydantic_parent_namespace__ = None\n |\n |  __pydantic_post_init__ = None\n |\n |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n |      Model...\n |\n |  __pydantic_validator__ = SchemaValidator(title=\"BaseModel\", validator=...\n |\n |  __signature__ = &lt;Signature () -&gt; None&gt;\n |\n |  model_config = {'alias_generator': &lt;function to_camel&gt;, 'arbitrary_typ...\n |\n |  ----------------------------------------------------------------------\n |  Methods inherited from pydantic.main.BaseModel:\n |\n |  __copy__(self) -&gt; 'Self'\n |      Returns a shallow copy of the model.\n |\n |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -&gt; 'Self'\n |      Returns a deep copy of the model.\n |\n |  __delattr__(self, item: 'str') -&gt; 'Any'\n |      Implement delattr(self, name).\n |\n |  __eq__(self, other: 'Any') -&gt; 'bool'\n |      Return self==value.\n |\n |  __getattr__(self, item: 'str') -&gt; 'Any'\n |\n |  __getstate__(self) -&gt; 'dict[Any, Any]'\n |      Helper for pickle.\n |\n |  __init__(self, /, **data: 'Any') -&gt; 'None'\n |      Create a new model by parsing and validating input data from keyword arguments.\n |\n |      Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be\n |      validated to form a valid model.\n |\n |      `self` is explicitly positional-only to allow `self` as a field name.\n |\n |  __iter__(self) -&gt; 'TupleGenerator'\n |      So `dict(model)` works.\n |\n |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -&gt; 'typing.Generator[Any, None, None]' from pydantic._internal._repr.Representation\n |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n |\n |  __replace__(self, **changes: 'Any') -&gt; 'Self'\n |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n |\n |  __repr__(self) -&gt; 'str'\n |      Return repr(self).\n |\n |  __repr_args__(self) -&gt; '_repr.ReprArgs'\n |\n |  __repr_name__(self) -&gt; 'str' from pydantic._internal._repr.Representation\n |      Name of the instance's class, used in __repr__.\n |\n |  __repr_recursion__(self, object: 'Any') -&gt; 'str' from pydantic._internal._repr.Representation\n |      Returns the string representation of a recursive object.\n |\n |  __repr_str__(self, join_str: 'str') -&gt; 'str' from pydantic._internal._repr.Representation\n |\n |  __rich_repr__(self) -&gt; 'RichReprResult' from pydantic._internal._repr.Representation\n |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n |\n |  __setattr__(self, name: 'str', value: 'Any') -&gt; 'None'\n |      Implement setattr(self, name, value).\n |\n |  __setstate__(self, state: 'dict[Any, Any]') -&gt; 'None'\n |\n |  __str__(self) -&gt; 'str'\n |      Return str(self).\n |\n |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -&gt; 'Self'\n |      Returns a copy of the model.\n |\n |      !!! warning \"Deprecated\"\n |          This method is now deprecated; use `model_copy` instead.\n |\n |      If you need `include` or `exclude`, use:\n |\n |      ```python {test=\"skip\" lint=\"skip\"}\n |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n |      data = {**data, **(update or {})}\n |      copied = self.model_validate(data)\n |      ```\n |\n |      Args:\n |          include: Optional set or mapping specifying which fields to include in the copied model.\n |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n |\n |      Returns:\n |          A copy of the model with included, excluded and updated fields as specified.\n |\n |  dict(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False) -&gt; 'Dict[str, Any]'\n |\n |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -&gt; 'str'\n |\n |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -&gt; 'Self'\n |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#model_copy\n |\n |      Returns a copy of the model.\n |\n |      Args:\n |          update: Values to change/add in the new model. Note: the data is not validated\n |              before creating the new model. You should trust this data.\n |          deep: Set to `True` to make a deep copy of the model.\n |\n |      Returns:\n |          New model instance.\n |\n |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -&gt; 'dict[str, Any]'\n |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump\n |\n |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n |\n |      Args:\n |          mode: The mode in which `to_python` should run.\n |              If mode is 'json', the output will only contain JSON serializable types.\n |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n |          include: A set of fields to include in the output.\n |          exclude: A set of fields to exclude from the output.\n |          context: Additional context to pass to the serializer.\n |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n |          exclude_defaults: Whether to exclude fields that are set to their default value.\n |          exclude_none: Whether to exclude fields that have a value of `None`.\n |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n |\n |      Returns:\n |          A dictionary representation of the model.\n |\n |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, serialize_as_any: 'bool' = False) -&gt; 'str'\n |      Usage docs: https://docs.pydantic.dev/2.10/concepts/serialization/#modelmodel_dump_json\n |\n |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n |\n |      Args:\n |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n |          include: Field(s) to include in the JSON output.\n |          exclude: Field(s) to exclude from the JSON output.\n |          context: Additional context to pass to the serializer.\n |          by_alias: Whether to serialize using field aliases.\n |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n |          exclude_defaults: Whether to exclude fields that are set to their default value.\n |          exclude_none: Whether to exclude fields that have a value of `None`.\n |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n |\n |      Returns:\n |          A JSON string representation of the model.\n |\n |  model_post_init(self, _BaseModel__context: 'Any') -&gt; 'None'\n |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n |      This is useful if you want to do some validation that requires the entire model to be initialized.\n |\n |  ----------------------------------------------------------------------\n |  Class methods inherited from pydantic.main.BaseModel:\n |\n |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -&gt; 'type[BaseModel] | _forward_ref.PydanticRecursiveRef'\n |\n |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -&gt; 'CoreSchema'\n |      Hook into generating the model's CoreSchema.\n |\n |      Args:\n |          source: The class we are generating a schema for.\n |              This will generally be the same as the `cls` argument if this is a classmethod.\n |          handler: A callable that calls into Pydantic's internal CoreSchema generation logic.\n |\n |      Returns:\n |          A `pydantic-core` `CoreSchema`.\n |\n |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -&gt; 'JsonSchemaValue'\n |      Hook into generating the model's JSON schema.\n |\n |      Args:\n |          core_schema: A `pydantic-core` CoreSchema.\n |              You can ignore this argument and call the handler with a new CoreSchema,\n |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n |              or just call the handler with the original schema.\n |          handler: Call into Pydantic's internal JSON schema generation.\n |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n |              generation fails.\n |              Since this gets called by `BaseModel.model_json_schema` you can override the\n |              `schema_generator` argument to that function to change JSON schema generation globally\n |              for a type.\n |\n |      Returns:\n |          A JSON schema, as a Python object.\n |\n |  __pydantic_init_subclass__(**kwargs: 'Any') -&gt; 'None'\n |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n |      be present when this is called.\n |\n |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n |\n |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n |      any kwargs passed to the class definition that aren't used internally by pydantic.\n |\n |      Args:\n |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n |              by pydantic.\n |\n |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -&gt; 'Self'\n |\n |  from_orm(obj: 'Any') -&gt; 'Self'\n |\n |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -&gt; 'Self'\n |      Creates a new instance of the `Model` class with validated data.\n |\n |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n |      Default values are respected, but no other validation is performed.\n |\n |      !!! note\n |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n |          an error if extra values are passed, but they will be ignored.\n |\n |      Args:\n |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n |              Otherwise, the field names from the `values` argument will be used.\n |          values: Trusted or pre-validated data dictionary.\n |\n |      Returns:\n |          A new instance of the `Model` class with validated data.\n |\n |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = &lt;class 'pydantic.json_schema.GenerateJsonSchema'&gt;, mode: 'JsonSchemaMode' = 'validation') -&gt; 'dict[str, Any]'\n |      Generates a JSON schema for a model class.\n |\n |      Args:\n |          by_alias: Whether to use attribute aliases or not.\n |          ref_template: The reference template.\n |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n |              `GenerateJsonSchema` with your desired modifications\n |          mode: The mode in which to generate the schema.\n |\n |      Returns:\n |          The JSON schema for the given model class.\n |\n |  model_parametrized_name(params: 'tuple[type[Any], ...]') -&gt; 'str'\n |      Compute the class name for parametrizations of generic classes.\n |\n |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n |\n |      Args:\n |          params: Tuple of types of the class. Given a generic class\n |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n |              the value `(str, int)` would be passed to `params`.\n |\n |      Returns:\n |          String representing the new class where `params` are passed to `cls` as type variables.\n |\n |      Raises:\n |          TypeError: Raised when trying to generate concrete names for non-generic models.\n |\n |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -&gt; 'bool | None'\n |      Try to rebuild the pydantic-core schema for the model.\n |\n |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n |      the initial attempt to build the schema, and automatic rebuilding fails.\n |\n |      Args:\n |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n |          raise_errors: Whether to raise errors, defaults to `True`.\n |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n |          _types_namespace: The types namespace, defaults to `None`.\n |\n |      Returns:\n |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n |\n |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None) -&gt; 'Self'\n |      Validate a pydantic model instance.\n |\n |      Args:\n |          obj: The object to validate.\n |          strict: Whether to enforce types strictly.\n |          from_attributes: Whether to extract data from object attributes.\n |          context: Additional context to pass to the validator.\n |\n |      Raises:\n |          ValidationError: If the object could not be validated.\n |\n |      Returns:\n |          The validated model instance.\n |\n |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None) -&gt; 'Self'\n |      Usage docs: https://docs.pydantic.dev/2.10/concepts/json/#json-parsing\n |\n |      Validate the given JSON data against the Pydantic model.\n |\n |      Args:\n |          json_data: The JSON data to validate.\n |          strict: Whether to enforce types strictly.\n |          context: Extra variables to pass to the validator.\n |\n |      Returns:\n |          The validated Pydantic model.\n |\n |      Raises:\n |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n |\n |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None) -&gt; 'Self'\n |      Validate the given object with string data against the Pydantic model.\n |\n |      Args:\n |          obj: The object containing string data to validate.\n |          strict: Whether to enforce types strictly.\n |          context: Extra variables to pass to the validator.\n |\n |      Returns:\n |          The validated Pydantic model.\n |\n |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -&gt; 'Self'\n |\n |  parse_obj(obj: 'Any') -&gt; 'Self'\n |\n |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -&gt; 'Self'\n |\n |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -&gt; 'Dict[str, Any]'\n |\n |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -&gt; 'str'\n |\n |  update_forward_refs(**localns: 'Any') -&gt; 'None'\n |\n |  validate(value: 'Any') -&gt; 'Self'\n |\n |  ----------------------------------------------------------------------\n |  Readonly properties inherited from pydantic.main.BaseModel:\n |\n |  __fields_set__\n |\n |  model_computed_fields\n |      Get metadata about the computed fields defined on the model.\n |\n |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n |      In V3, this property will be removed from the `BaseModel` class.\n |\n |      Returns:\n |          A mapping of computed field names to [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.\n |\n |  model_extra\n |      Get extra fields set during validation.\n |\n |      Returns:\n |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n |\n |  model_fields\n |      Get metadata about the fields defined on the model.\n |\n |      Deprecation warning: you should be getting this information from the model class, not from an instance.\n |      In V3, this property will be removed from the `BaseModel` class.\n |\n |      Returns:\n |          A mapping of field names to [`FieldInfo`][pydantic.fields.FieldInfo] objects.\n |\n |  model_fields_set\n |      Returns the set of fields that have been explicitly set on this model instance.\n |\n |      Returns:\n |          A set of strings representing the fields that have been set,\n |              i.e. that were not filled from defaults.\n |\n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pydantic.main.BaseModel:\n |\n |  __dict__\n |      dictionary for instance variables\n |\n |  __pydantic_extra__\n |\n |  __pydantic_fields_set__\n |\n |  __pydantic_private__\n |\n |  ----------------------------------------------------------------------\n |  Data and other attributes inherited from pydantic.main.BaseModel:\n |\n |  __hash__ = None\n |\n |  __pydantic_root_model__ = False\n\n\n\nAll the models in genai.types that are then used to cast the interaction back and forth with the API are subclasses of the genai._common.BaseModel. Although this being a “private” module make it less than ideal, it will be useful to quickly monkey patch a better representation of the output.\n\nStopping sequences, system prompts, and streaming\n\ntypes.GenerateContentConfigDict??\n\n\nInit signature: types.GenerateContentConfigDict(self, /, *args, **kwargs)\nSource:        \nclass GenerateContentConfigDict(TypedDict, total=False):\n  \"\"\"Optional model configuration parameters.\n  For more information, see `Content generation parameters\n  &lt;https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/content-generation-parameters&gt;`_.\n  \"\"\"\n  http_options: Optional[HttpOptionsDict]\n  \"\"\"Used to override HTTP request options.\"\"\"\n  system_instruction: Optional[ContentUnionDict]\n  \"\"\"Instructions for the model to steer it toward better performance.\n      For example, \"Answer as concisely as possible\" or \"Don't use technical\n      terms in your response\".\n      \"\"\"\n  temperature: Optional[float]\n  \"\"\"Value that controls the degree of randomness in token selection.\n      Lower temperatures are good for prompts that require a less open-ended or\n      creative response, while higher temperatures can lead to more diverse or\n      creative results.\n      \"\"\"\n  top_p: Optional[float]\n  \"\"\"Tokens are selected from the most to least probable until the sum\n      of their probabilities equals this value. Use a lower value for less\n      random responses and a higher value for more random responses.\n      \"\"\"\n  top_k: Optional[float]\n  \"\"\"For each token selection step, the ``top_k`` tokens with the\n      highest probabilities are sampled. Then tokens are further filtered based\n      on ``top_p`` with the final token selected using temperature sampling. Use\n      a lower number for less random responses and a higher number for more\n      random responses.\n      \"\"\"\n  candidate_count: Optional[int]\n  \"\"\"Number of response variations to return.\n      \"\"\"\n  max_output_tokens: Optional[int]\n  \"\"\"Maximum number of tokens that can be generated in the response.\n      \"\"\"\n  stop_sequences: Optional[list[str]]\n  \"\"\"List of strings that tells the model to stop generating text if one\n      of the strings is encountered in the response.\n      \"\"\"\n  response_logprobs: Optional[bool]\n  \"\"\"Whether to return the log probabilities of the tokens that were\n      chosen by the model at each step.\n      \"\"\"\n  logprobs: Optional[int]\n  \"\"\"Number of top candidate tokens to return the log probabilities for\n      at each generation step.\n      \"\"\"\n  presence_penalty: Optional[float]\n  \"\"\"Positive values penalize tokens that already appear in the\n      generated text, increasing the probability of generating more diverse\n      content.\n      \"\"\"\n  frequency_penalty: Optional[float]\n  \"\"\"Positive values penalize tokens that repeatedly appear in the\n      generated text, increasing the probability of generating more diverse\n      content.\n      \"\"\"\n  seed: Optional[int]\n  \"\"\"When ``seed`` is fixed to a specific number, the model makes a best\n      effort to provide the same response for repeated requests. By default, a\n      random number is used.\n      \"\"\"\n  response_mime_type: Optional[str]\n  \"\"\"Output response media type of the generated candidate text.\n      \"\"\"\n  response_schema: Optional[SchemaUnionDict]\n  \"\"\"Schema that the generated candidate text must adhere to.\n      \"\"\"\n  routing_config: Optional[GenerationConfigRoutingConfigDict]\n  \"\"\"Configuration for model router requests.\n      \"\"\"\n  safety_settings: Optional[list[SafetySettingDict]]\n  \"\"\"Safety settings in the request to block unsafe content in the\n      response.\n      \"\"\"\n  tools: Optional[ToolListUnionDict]\n  \"\"\"Code that enables the system to interact with external systems to\n      perform an action outside of the knowledge and scope of the model.\n      \"\"\"\n  tool_config: Optional[ToolConfigDict]\n  \"\"\"Associates model output to a specific function call.\n      \"\"\"\n  labels: Optional[dict[str, str]]\n  \"\"\"Labels with user-defined metadata to break down billed charges.\"\"\"\n  cached_content: Optional[str]\n  \"\"\"Resource name of a context cache that can be used in subsequent\n      requests.\n      \"\"\"\n  response_modalities: Optional[list[str]]\n  \"\"\"The requested modalities of the response. Represents the set of\n      modalities that the model can return.\n      \"\"\"\n  media_resolution: Optional[MediaResolution]\n  \"\"\"If specified, the media resolution specified will be used.\n    \"\"\"\n  speech_config: Optional[SpeechConfigUnionDict]\n  \"\"\"The speech generation configuration.\n      \"\"\"\n  audio_timestamp: Optional[bool]\n  \"\"\"If enabled, audio timestamp will be included in the request to the\n       model.\n      \"\"\"\n  automatic_function_calling: Optional[AutomaticFunctionCallingConfigDict]\n  \"\"\"The configuration for automatic function calling.\n      \"\"\"\n  thinking_config: Optional[ThinkingConfigDict]\n  \"\"\"The thinking features configuration.\n      \"\"\"\nFile:           ~/mambaforge/envs/prototypes/lib/python3.12/site-packages/google/genai/types.py\nType:           _TypedDictMeta\nSubclasses:     \n\n\n\nAll the generation paramters can be passed as a dictionary. The available parameters are defined by the GenerateContentConfigDict model (which is just a snake case/dictionary conversion of GenerateContentConfig). This include things like temperature and top_k/top_p value as well as stopping sequences and system prompt (which is actually passed at each generation)\n\nsr = c.models.generate_content(model=model, \n                               contents=\"Count from 1 to 5 and add a write a different animal after each number\",\n                               config={\"stop_sequences\": [\"4\"]})\nprint(sr.text)\n\nOkay, here we go!\n\n1 - Cat\n2 - Dog\n3 - Bird\n\n\n\n\nspr = c.models.generate_content(model=model, \n                               contents=\"Count from 1 to 5 and add a write a different animal after each number\",\n                               config={\"system_instruction\": \"Always talk in Spanish\"})\nprint(spr.text)\n\n¡Por supuesto! Aquí tienes:\n\n1.  Uno... Perro\n2.  Dos... Gato\n3.  Tres... Elefante\n4.  Cuatro... León\n5.  Cinco... Ballena\n\n\n\n\nfor chunk in c.models.generate_content_stream(model=model, contents=\"Write a small poem about the hardships of being a cocker spaniel\"):\n    print(chunk.text, end='')\n\nWith ears so long, a tripping hazard,\nThrough muddy fields, a muddy blaggard.\nA feathered tail, a joyful wag,\nYet burrs collect, a constant snag.\n\nMy soulful eyes, they plead and yearn,\nFor walks and treats, a love to earn.\nBut oh, the groomer, shears so bright,\nA cocker's life, a fluffy fight.\n\n\n\ntype(chunk)\n\ngoogle.genai.types.GenerateContentResponse\n\n\nStreaming content is handled with a separate method of models but everything else stays the same.\n\n\nImage generation\n\nmr = c.models.generate_images(\n    model = imagen_models[0],\n    prompt = \"A roman mosaic of a boxing match between a feathered dinosaur and a cocker spaniel,\\\nrefereed by Sasquatch, in front of a crowd of cheering otters.\",\n    config = {\"number_of_images\": 2}\n)\n\n\ntype(mr)\n\ngoogle.genai.types.GenerateImagesResponse\n\n\nCurrently the only multimedia output supported is image generation via imagen models (although audio and image generation with Gemini 2.0 is currently being tested in restricted availability). This uses a separate method, and returns a different type of response. The possible options are defined in the pydantic model but not all of them are actually available (for example, the enhance_prompt option does not work with the Gemini API).\n\nfor genim in  mr.generated_images:\n    genim.image.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmr.generated_images[0].image.save('match.png')\n\nAs usual, the response is a convoluted nested mess of pydantic models. The images are returned as a wrapper of a PIL.Image which gives access to a few convenience function for displaying and saving. Right now jpeg is the only possible output format.",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "00_core.html#formatting-output",
    "href": "00_core.html#formatting-output",
    "title": "gaspare",
    "section": "Formatting Output",
    "text": "Formatting Output\n\nr.text\n\n'Yes, I am ready to work! What can I do for you?\\n'\n\n\n\nr.to_json_dict()\n\n{'candidates': [{'content': {'parts': [{'text': 'Yes, I am ready to work! What can I do for you?\\n'}],\n    'role': 'model'},\n   'avg_logprobs': -0.36470311880111694,\n   'finish_reason': 'STOP'}],\n 'model_version': 'gemini-2.0-flash',\n 'usage_metadata': {'candidates_token_count': 16,\n  'prompt_token_count': 9,\n  'total_token_count': 25},\n 'automatic_function_calling_history': []}\n\n\n\nr.model_fields_set\n\n{'automatic_function_calling_history',\n 'candidates',\n 'model_version',\n 'usage_metadata'}\n\n\n\ntype(r.usage_metadata), type(r.candidates), type(r.model_version)\n\n(google.genai.types.GenerateContentResponseUsageMetadata, list, str)\n\n\nWe want to recursively navigate the nested tree of submodels and attributes. Each model in genai.types has three types of attributes:\n\na genai.types module (like usage_metadata)\na list (like candidates)\na primitive type (like model_version)\n\nWe could extract the attributes of a model from the class itself, but to avoid cluttering the output, we can use the model_fields_set property to avoid attributes that are not set in the model instance.\n\nsource\n\nget_repr\n\n get_repr (m, lab='')\n\nRecurisvely fetch the markdown representation of genai.types fields, wrapping lists into &lt;details&gt; blocks\n\n\nExported source\ndef get_repr(m, lab=\"\"):\n    \"\"\"Recurisvely fetch the markdown representation of genai.types fields, wrapping lists into `&lt;details&gt;` blocks\"\"\"\n    if hasattr(m, '_repr_markdown_'): return m._repr_markdown_()\n    if is_listy(m): return \"\\n\".join([f\"&lt;details open='true'&gt;&lt;summary&gt;{lab}[{i}]&lt;/summary&gt;{get_repr(li)}&lt;/details&gt;\" for i, li in enumerate(m)])\n    if isinstance(m, dict): return \"&lt;ul&gt;\" + \"\\n\".join([f\"&lt;li&gt;&lt;b&gt;{i}&lt;/b&gt;: {get_repr(li, i)}&lt;/li&gt;\" for i, li in m.items()]) + \"&lt;/ul&gt;\"\n    if isinstance(m, bytes): return m[:10] + b'...'\n    return str(m)\n\n\n\nMarkdown(get_repr([[\"A\", \"B\", \"C\"], \"b\", \"c\", {\"x\": 2, \"y\": {\"as\": \"sa\"}}], \"ex\"))\n\n\n\nex[0]\n\n\n\n[0]\n\nA\n\n\n\n[1]\n\nB\n\n\n\n[2]\n\nC\n\n\n\n\nex[1]\n\nb\n\n\n\nex[2]\n\nc\n\n\n\nex[3]\n\n\n\nx: 2\n\n\ny:\n\n\nas: sa\n\n\n\n\n\n\n\nThe basic recursive loop is in place: we can handle genai.types (via their _repr_markdown_ methods), strings and lists. The handling of bytes is to avoid polluting (or crashing) the representation with a huge list of charachters in case of a multimodal response.\n\nsource\n\n\ndet_repr\n\n det_repr (m)\n\n\n\nExported source\ndef det_repr(m): return \"&lt;ul&gt;\" + \"\".join(f\"&lt;li&gt;&lt;code&gt;{d}&lt;/code&gt;: {get_repr(getattr(m, d), d)}&lt;/li&gt;\" for d in m.model_fields_set) + \"&lt;/ul&gt;\"\n\n\n\nMarkdown(det_repr(r.usage_metadata))\n\n\n\ncandidates_token_count: 16\n\n\ntotal_token_count: 25\n\n\nprompt_token_count: 9\n\n\n\n\nWrapping the details in a list makes for a cleaner and more readable look.\n\n\nExported source\n@patch\ndef _repr_markdown_(self: genai._common.BaseModel):\n    return det_repr(self)\n\n\n\nr\n\n\n\nusage_metadata:\n\n\ncandidates_token_count: 16\n\n\ntotal_token_count: 25\n\n\nprompt_token_count: 9\n\n\n\n\nmodel_version: gemini-2.0-flash\n\n\ncandidates:\n\n\ncandidates[0]\n\n\n\navg_logprobs: -0.36470311880111694\n\n\ncontent:\n\n\nparts:\n\n\nparts[0]\n\n\n\ntext: Yes, I am ready to work! What can I do for you?\n\n\n\n\n\nrole: model\n\n\n\n\nfinish_reason: FinishReason.STOP\n\n\n\n\n\nautomatic_function_calling_history:\n\n\n\n\nBy using fastcore’s patch on the _common.BaseModel we have made sure that all the models in genai.types have a nice consistent markdown representation. We can now refine the representation for some of the types.\n\nResponse representation\n\n\nExported source\n@patch\ndef _repr_markdown_(self: genai.types.GenerateContentResponse):\n    c = None\n    try:\n        c = self.text.replace(\"\\n\", \"&lt;br /&gt;\")\n    except ValueError as e:\n        calls = (f\"&lt;code&gt;{call.name}({', '.join([f'{a}={v}' for a, v in call.args.items()])})&lt;/code&gt;\" for call in self.function_calls)\n        calls_repr = '\\n'.join(f'&lt;li&gt;{c}&lt;/li&gt;' for c in calls)\n        c = f\"&lt;ul&gt;{calls_repr}&lt;/ul&gt;\"\n    dets = det_repr(self)\n    return f\"\"\"{c}\\n&lt;details&gt;{dets}&lt;/details&gt;\"\"\"\n\n\n\nr\n\nYes, I am ready to work! What can I do for you?\n\n\n\nusage_metadata: Cached: 0; In: 9; Out: 16; Total: 25\n\n\nmodel_version: gemini-2.0-flash\n\n\ncandidates:\n\n\ncandidates[0]\n\n\n\navg_logprobs: -0.36470311880111694\n\n\ncontent:\n\n\nparts:\n\n\nparts[0]\n\n\n\ntext: Yes, I am ready to work! What can I do for you?\n\n\n\n\n\nrole: model\n\n\n\n\nfinish_reason: FinishReason.STOP\n\n\n\n\n\nautomatic_function_calling_history:\n\n\n\n\n\n\n\nMultimodal response representation\n\nmr\n\n\n\ngenerated_images:\n\n\ngenerated_images[0]\n\n\n\nimage:\n\n\nmime_type: image/png\n\n\nimage_bytes: b’89PNG1a…’\n\n\n\n\n\n\n\ngenerated_images[1]\n\n\n\nimage:\n\n\nmime_type: image/png\n\n\nimage_bytes: b’89PNG1a…’\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage.html\n\n Image.html ()\n\n\n\nExported source\n@patch(as_prop=True)\ndef html(self: types.Image):\n    b64 = base64.b64encode(self.image_bytes).decode(\"utf-8\")\n    return f'&lt;img src=\"data:{self.mime_type};base64,{b64}\" /&gt;'\n\n\n@patch\ndef _repr_markdown_(self: types.Image):\n    return f\"\"\"&lt;div style=\"width: 100px; height: auto;\"&gt;{self.html}&lt;/div&gt;\n\n&lt;details&gt;\n{det_repr(self)}\n&lt;/details&gt;\"\"\"\n\n\n\nmr.generated_images[0].image\n\n\n\n\n\n\n\nmime_type: image/png\n\n\nimage_bytes: b’89PNG1a…’\n\n\n\n\n\n\n\n\nGenerateImagesResponse.img\n\n GenerateImagesResponse.img ()\n\n\n\nExported source\n@patch(as_prop=True)\ndef img(self: types.GenerateImagesResponse):\n    return self.generated_images[0].image._pil_image\n\n\n@patch\ndef _repr_markdown_(self: types.GenerateImagesResponse):\n    N = len(self.generated_images)\n    cols = min(N, 4)\n    rows = math.ceil(N / 4)\n   \n    ims = \"\".join([f\"\"\"&lt;div style=\"display: grid; \n                    width: 100%; \n                    max-width: 1000px; \n                    height: auto;\n                    margin: 0 auto; \n                    grid-template-columns: {rows}fr;\n                    grid-template-rows: 1ft;\n                   \"&gt;{gim.image.html}&lt;/div&gt;\"\"\" \n                   for gim in self.generated_images])\n    \n    \n    \n    i = f\"\"\"\n&lt;div style=\"display: grid; \n                gap: 4px; \n                width: 100%;\n                height: auto;\n                max-width: 1000px; \n                margin: 0 auto; \n                padding: 4px;\n                grid-template-columns: repeat({cols}, 1fr);\n                grid-template-rows: repeat({rows}, 1fr);\n                \"&gt;\n\n{ims}\n\n&lt;/div&gt;\n    \"\"\"\n    return f\"\"\"{i}\n\n&lt;details&gt;\n{det_repr(self)}\n&lt;/details&gt;\n\"\"\"\n\n\n\nmr.img\n\n\n\n\n\n\n\n\n\nmr\n\n\n\n\n\n\n\n\n\n\n\n\ngenerated_images:\n\n\ngenerated_images[0]\n\n\n\nimage:\n\n\n\n\n\n\nmime_type: image/png\n\n\nimage_bytes: b’89PNG1a…’\n\n\n\n\n\n\n\n\ngenerated_images[1]\n\n\n\nimage:\n\n\n\n\n\n\nmime_type: image/png\n\n\nimage_bytes: b’89PNG1a…’\n\n\n\n\n\n\n\n\n\n\n\nMost of the time we probably return a single image and .img property gives a convenient way of fetching the PIL image. The markdown representation is not ideal, but turns out that sizing a variable number of images into a grid can be very tricky, so we’ll leave it at that for the moment.",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "00_core.html#usage-query-costs",
    "href": "00_core.html#usage-query-costs",
    "title": "gaspare",
    "section": "Usage & query costs",
    "text": "Usage & query costs\n\nsource\n\nusage\n\n usage (inp=0, out=0, cached=0)\n\nA quicker and simpler constructor for the Usage Metadata model\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninp\nint\n0\nNumber of input tokens (excluding cached)\n\n\nout\nint\n0\nNumber of output tokens\n\n\ncached\nint\n0\nNumber of cached tokens\n\n\n\n\n\nExported source\ndef usage(inp=0,     # Number of input tokens (excluding cached)\n          out=0,     # Number of output tokens\n          cached=0): # Number of cached tokens\n    \"\"\"A quicker and simpler constructor for the Usage Metadata model\"\"\"\n    return types.GenerateContentResponseUsageMetadata(cached_content_token_count=cached, \n                                                      candidates_token_count=out, \n                                                      prompt_token_count=inp + cached, \n                                                      total_token_count=inp + out + cached)\n\n\nUnusually, prompt_token_count includes both cached and uncached prompt tokens.\n\nusage(0, 32, 12)\n\n\n\ncandidates_token_count: 32\n\n\ntotal_token_count: 44\n\n\nprompt_token_count: 12\n\n\ncached_content_token_count: 12\n\n\n\n\nAs usual, constructor for models are very verbose, so we build a simpler version.\n\n\n\nGenerateContentResponseUsageMetadata.total\n\n GenerateContentResponseUsageMetadata.total ()\n\n\n\nExported source\n@patch(as_prop=True)\ndef cached(self: types.GenerateContentResponseUsageMetadata): \n    return self.cached_content_token_count or 0\n\n@patch(as_prop=True)\ndef inp(self: types.GenerateContentResponseUsageMetadata): \n    return (self.prompt_token_count - self.cached) or 0\n\n@patch(as_prop=True)\ndef out(self: types.GenerateContentResponseUsageMetadata): \n    return self.candidates_token_count or 0\n\n@patch(as_prop=True)\ndef total(self: types.GenerateContentResponseUsageMetadata): \n    return self.total_token_count or self.prompt_token_count + self.candidates_token_count\n\n\n\n\n\nGenerateContentResponseUsageMetadata.out\n\n GenerateContentResponseUsageMetadata.out ()\n\n\n\n\nGenerateContentResponseUsageMetadata.inp\n\n GenerateContentResponseUsageMetadata.inp ()\n\n\n\n\nGenerateContentResponseUsageMetadata.cached\n\n GenerateContentResponseUsageMetadata.cached ()\n\n\nu = usage(1, 2, 3)\nu.inp, u.out, u.cached, u.total\n\n(1, 2, 3, 6)\n\n\nWe patch a few properties to make dealing with the usage object a bit less verbose.\n\n\n\nGenerateContentResponseUsageMetadata.__repr__\n\n GenerateContentResponseUsageMetadata.__repr__ ()\n\n\n\nExported source\n@patch\ndef __repr__(self: types.GenerateContentResponseUsageMetadata):\n    return f\"Cached: {self.cached}; In: {self.inp}; Out: {self.out}; Total: {self.total}\"\n\n@patch\ndef _repr_markdown_(self: types.GenerateContentResponseUsageMetadata):\n    return self.__repr__()\n\n\n\nu\n\nCached: 3; In: 1; Out: 2; Total: 6\n\n\nFinally, we make the string and markdown representation a bit more readable and coherent with the ones in claudette. Since we patched _repr_markdown_ in the BaseModel, we need to “unpatch” it here.\n\nr\n\nYes, I am ready to work! What can I do for you?\n\n\n\nusage_metadata: Cached: 0; In: 9; Out: 16; Total: 25\n\n\nmodel_version: gemini-2.0-flash\n\n\ncandidates:\n\n\ncandidates[0]\n\n\n\navg_logprobs: -0.36470311880111694\n\n\ncontent:\n\n\nparts:\n\n\nparts[0]\n\n\n\ntext: Yes, I am ready to work! What can I do for you?\n\n\n\n\n\nrole: model\n\n\n\n\nfinish_reason: FinishReason.STOP\n\n\n\n\n\nautomatic_function_calling_history:\n\n\n\n\n\n\n\n\nGenerateContentResponseUsageMetadata.__add__\n\n GenerateContentResponseUsageMetadata.__add__ (other)\n\n\n\nExported source\n@patch\ndef __add__(self: types.GenerateContentResponseUsageMetadata, other):\n    cached = getattr(self, \"cached\", 0) + getattr(other, \"cached\", 0)\n    return usage(self.inp + other.inp, self.out + other.out, cached)\n\n\n\nusage(5, 1) + usage(32, 32, 32)\n\nCached: 32; In: 37; Out: 33; Total: 102\n\n\n\n\nPricings\n\nsource\n\n\nget_pricing\n\n get_pricing (model, prompt_tokens)\n\n\n\nExported source\n# $/1M input (non cached) tokens, $/1M output tokens, $/1M cached input tokens, \n\npricings = {\n    'gemini-2.0-flash': [0.1, 0.4, 0.025],\n    'gemini-2.0-flash-lite': [0.075, 0.3, 0.01875],\n    'gemini-1.5-flash_short': [0.075, 0.3, 0.01875],\n    'gemini-1.5-flash_long': [0.15, 0.6, 0.0375], \n    'gemini-1.5-flash-8b_short': [0.0375, 0.15, 0.01],\n    'gemini-1.5-flash-8b_long': [0.075, 0.3, 0.02],\n    'gemini-1.5-pro_short': [1.25, 5., 0.3125],   \n    'gemini-1.5-pro_long': [2.5, 10., 0.625],\n }\n\n\naudio_token_pricings = {\n    'gemini-2.0-flash': [0.7, 0.4, 0.175],\n}\n\ndef get_pricing(model, prompt_tokens):\n    if \"-exp-\" in model: return [0, 0, 0]\n    suff = \"_long\" if prompt_tokens &gt; 128_000 else \"_short\"\n    m = all_model_types.get(model, \"#\").split(\"#\")[-1]\n    m += suff if \"1.5\" in m else \"\"\n    return pricings.get(m, [0, 0, 0])\n\n\nThe pricing of Gemini model queries is quite byzantine, with the price of a query on Gemini 1.5 dependent on the prompt length, while for Gemini 2.0 models it depends on the input type.\nA few things to notice:\n\nThe differential pricings for audio tokens for Gemini 2.0 Flash is not implemented (and the cost for audio tokens caching will be active starting Feb 24, 2025)\nCaching costs are not only per query. There is an added cost of storing the cache, which is computed separately and depends on the cached content storage time, as well as the number of tokens\n\nTODO: for the moment we are ignoring these nuances in the cost calculations, but we might want to do more precise computations at a later date (and maybe include cost of multimedia generation with Gemini 2, when it becomes available).\n\nfor m in models:\n    print(m, \"SHORT PROMPT\", get_pricing(m , 1_000))\n    print(m, \"LONG PROMPT\", get_pricing(m , 1_000_000))\n\ngemini-2.0-flash SHORT PROMPT [0.1, 0.4, 0.025]\ngemini-2.0-flash LONG PROMPT [0.1, 0.4, 0.025]\ngemini-2.0-flash-001 SHORT PROMPT [0.1, 0.4, 0.025]\ngemini-2.0-flash-001 LONG PROMPT [0.1, 0.4, 0.025]\ngemini-2.0-pro-exp-02-05 SHORT PROMPT [0, 0, 0]\ngemini-2.0-pro-exp-02-05 LONG PROMPT [0, 0, 0]\ngemini-2.0-flash-lite-preview-02-05 SHORT PROMPT [0.075, 0.3, 0.01875]\ngemini-2.0-flash-lite-preview-02-05 LONG PROMPT [0.075, 0.3, 0.01875]\ngemini-1.5-flash SHORT PROMPT [0.075, 0.3, 0.01875]\ngemini-1.5-flash LONG PROMPT [0.15, 0.6, 0.0375]\ngemini-1.5-pro SHORT PROMPT [1.25, 5.0, 0.3125]\ngemini-1.5-pro LONG PROMPT [2.5, 10.0, 0.625]\ngemini-1.5-pro-002 SHORT PROMPT [1.25, 5.0, 0.3125]\ngemini-1.5-pro-002 LONG PROMPT [2.5, 10.0, 0.625]\ngemini-1.5-flash-8b SHORT PROMPT [0.0375, 0.15, 0.01]\ngemini-1.5-flash-8b LONG PROMPT [0.075, 0.3, 0.02]\ngemini-2.0-flash-thinking-exp-01-21 SHORT PROMPT [0, 0, 0]\ngemini-2.0-flash-thinking-exp-01-21 LONG PROMPT [0, 0, 0]\n\n\n\n\n\nGenerateContentResponse.cost\n\n GenerateContentResponse.cost ()\n\n\n\nExported source\n@patch(as_prop=True)\ndef cost(self: types.GenerateContentResponse):\n    ip, op, cp = get_pricing(self.model_version, self.usage_metadata.prompt_token_count)\n    return ((self.usage_metadata.inp * ip) + (self.usage_metadata.out * op) + (self.usage_metadata.cached * cp)) / 1e6\n\n\n\nr.cost\n\n7.3e-06\n\n\n\n\n\nGenerateImagesResponse.cost\n\n GenerateImagesResponse.cost ()\n\n\n\nExported source\n@patch(as_prop=True)\ndef cost(self: types.GenerateImagesResponse): return 0.03 * len(self.generated_images)\n\n\n\nmr.cost\n\n0.06\n\n\nThere is some inconsistency in the pricing of Imagen models: according to this page it’s $0.03 per image generated, while according to the pricing page it’s $0.03 per million tokens (but there is no way of counting tokens on the image generated). Until clarified, we’ll stick with the former for simplicity.",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "00_core.html#client",
    "href": "00_core.html#client",
    "title": "gaspare",
    "section": "Client",
    "text": "Client\nWe actually don’t neeed to create a new client object from scratch. Thanks to fastcore we can actually extend the genai.Codels class to give it what we want. Namely:\n\nSimple multimodal prompt handling\nA simpler generation interface, coherent with claudette and cosette\nCost and usage tracking\n\nWe can the build on this to add capabilities for tool usage, multimodal outputs etc.\n\nCreating messages\nMessages sent to Gemini are made of a list of Parts, which can be text, or multimedia parts. Some multimedia files can be inlined as bytes (in particular images), but others need to be uploaded using the file API first. Although this is quite flexible, it’s a bit clunky, so we want to make it easier.\n\nsource\n\n\nmk_part\n\n mk_part (inp:Union[str,pathlib.Path,google.genai.types.Part,google.genai.\n          types.File,PIL.Image.Image],\n          c:google.genai.client.Client|None=None)\n\nTurns an input fragment into a multimedia Part to be sent to a Gemini model\n\n\nExported source\ndef mk_part(inp: Union[str, Path, types.Part, types.File, PIL.Image.Image], c: genai.Client|None=None):\n    \"Turns an input fragment into a multimedia `Part` to be sent to a Gemini model\"\n    api_client = c or genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n    if isinstance(inp, (types.Part, types.File, PIL.Image.Image)): return inp\n    p_inp = Path(inp)\n    if p_inp.exists():\n        mt = mimetypes.guess_type(p_inp)[0]\n        if mt.split(\"/\")[0] == \"image\": return types.Part.from_bytes(data=p_inp.read_bytes(), mime_type=mt)\n        return api_client.files.upload(file=p_inp)\n    return types.Part.from_text(text=inp)\n\n\nNotice that we cannot make mk_part a completely standalone function. Having access to the files API requires an a client. We could pass the genai.Client we have created before (or monkey patch mk_part into a class that has access to a client already), but for testing we create a new client each time.\n\nmk_part(\"Hello World\")\n\n\n\ntext: Hello World\n\n\n\n\n\npimg = mr.generated_images[0].image._pil_image\nmk_part(pimg)\n\n\n\n\n\n\n\n\n\n# This will take a bit of time, since the pdf needs to be uploaded\nf = mk_part(Path(\"DeepSeek_R1.pdf\"))\nf\n\n\n\nmime_type: application/pdf\n\n\nuri: https://generativelanguage.googleapis.com/v1beta/files/npm1fry2anf6\n\n\nname: files/npm1fry2anf6\n\n\nexpiration_time: 2025-02-16 22:23:14.562145+00:00\n\n\nupdate_time: 2025-02-14 22:23:14.712394+00:00\n\n\nsha256_hash: ZDEzNTQwZDY0MDA1ODY3YmNjYjZkMzljYWQ3NTU0NzQwYTJiYjZlOTc5NmU5YjQ2YWJjM2JhYTliNWI4OGZhZQ==\n\n\nstate: FileState.ACTIVE\n\n\nsize_bytes: 1326429\n\n\nsource: FileSource.UPLOADED\n\n\ncreate_time: 2025-02-14 22:23:14.712394+00:00\n\n\n\n\n\n# This should be instant\nmk_part(f)\n\n\n\nmime_type: application/pdf\n\n\nuri: https://generativelanguage.googleapis.com/v1beta/files/npm1fry2anf6\n\n\nname: files/npm1fry2anf6\n\n\nexpiration_time: 2025-02-16 22:23:14.562145+00:00\n\n\nupdate_time: 2025-02-14 22:23:14.712394+00:00\n\n\nsha256_hash: ZDEzNTQwZDY0MDA1ODY3YmNjYjZkMzljYWQ3NTU0NzQwYTJiYjZlOTc5NmU5YjQ2YWJjM2JhYTliNWI4OGZhZQ==\n\n\nstate: FileState.ACTIVE\n\n\nsize_bytes: 1326429\n\n\nsource: FileSource.UPLOADED\n\n\ncreate_time: 2025-02-14 22:23:14.712394+00:00\n\n\n\n\nTODO: Notice that we are not handling the case when the file is expired, but these should not be long term objects anyways.\n\nmk_part(\"match.png\")\n\n\n\ninline_data:\n\n\nmime_type: image/png\n\n\ndata: b’89PNG1a…’\n\n\n\n\n\n\nGenai already handles PIL images, and Files input fragment. We could call explicitly the genai._transformers.t_part (it’s actually imported as genai.models.t.t_part) function here to make sure that mk_part always returns a types.Part, but\n\nWhy do the extra work?\nUsing something from a “private” module like _transformers is probably not a great idea, although technically this is exposed as genai.models.t\n\n\nsource\n\n\nmk_parts\n\n mk_parts (inps, c=None)\n\n\n\nExported source\ndef is_texty(o): return isinstance(o, str) or (isinstance(o, types.Part) and bool(o.text))\n\ndef mk_parts(inps, c=None):\n    cts = L(inps).map(mk_part, c=c) if inps else L(\" \")\n    return list(cts) if len(cts) &gt; 1 or is_texty(cts[0]) else list(cts + [\" \"])\n\n\n\nsource\n\n\nis_texty\n\n is_texty (o)\n\nGemini does not like empty inputs or messages with just a single media file, so in those cases we append a string with a single space to the content. We need to convert back from L to actual list because the former triggers the type checking of the api.\n\ntry: a = c.models.generate_content(model=\"gemini-2.0-flash\", contents=\"\")\nexcept ValueError: b = c.models.generate_content(model=\"gemini-2.0-flash\", contents=mk_parts(\"\"))\nb\n\nPlease provide me with more context! I need to know what you’d like me to do with your request. For example, are you asking me to:* Write something? If so, what should I write about? What is the topic, the audience, and the desired tone?* Summarize something? If so, please provide the text you want me to summarize.* Answer a question? If so, what is the question?* Generate code? If so, what language should I use and what should the code do?* Translate something? If so, what language should I translate to and from, and what is the text to be translated?The more information you give me, the better I can understand your request and provide a helpful response.\n\n\n\nusage_metadata: Cached: 0; In: 1; Out: 174; Total: 175\n\n\nmodel_version: gemini-2.0-flash\n\n\ncandidates:\n\n\ncandidates[0]\n\n\n\navg_logprobs: -0.20099690316737384\n\n\ncontent:\n\n\nparts:\n\n\nparts[0]\n\n\n\ntext: Please provide me with more context! I need to know what you’d like me to do with your request. For example, are you asking me to:\n\nWrite something? If so, what should I write about? What is the topic, the audience, and the desired tone?\nSummarize something? If so, please provide the text you want me to summarize.\nAnswer a question? If so, what is the question?\nGenerate code? If so, what language should I use and what should the code do?\nTranslate something? If so, what language should I translate to and from, and what is the text to be translated?\n\nThe more information you give me, the better I can understand your request and provide a helpful response.\n\n\n\n\n\nrole: model\n\n\n\n\nfinish_reason: FinishReason.STOP\n\n\n\n\n\nautomatic_function_calling_history:\n\n\n\n\n\n\nparts = mk_parts([\"DeepSeek_R1.pdf\", \"What is this?\"], c=c)\n\nc.models.generate_content(model=\"gemini-2.0-flash\", contents=parts)\n\nThis is a research paper titled “DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning” by DeepSeek-AI. The paper introduces their first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1, and their training process using reinforcement learning. It also discusses distilling reasoning capabilities from DeepSeek-R1 to smaller dense models. The paper includes benchmark results for DeepSeek-R1 and its distilled versions, comparing them to other models like OpenAI’s models and open-source models.\n\n\n\nusage_metadata: Cached: 0; In: 5680; Out: 115; Total: 5795\n\n\nmodel_version: gemini-2.0-flash\n\n\ncandidates:\n\n\ncandidates[0]\n\n\n\navg_logprobs: -0.2893808779509171\n\n\ncontent:\n\n\nparts:\n\n\nparts[0]\n\n\n\ntext: This is a research paper titled “DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning” by DeepSeek-AI. The paper introduces their first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1, and their training process using reinforcement learning. It also discusses distilling reasoning capabilities from DeepSeek-R1 to smaller dense models. The paper includes benchmark results for DeepSeek-R1 and its distilled versions, comparing them to other models like OpenAI’s models and open-source models.\n\n\n\n\n\nrole: model\n\n\n\n\nfinish_reason: FinishReason.STOP\n\n\n\n\n\nautomatic_function_calling_history:\n\n\n\n\n\n\n\nGeneration interface\nWe start with a simple definition of __call__ on the client. Adding functionalities a bit at a time.\n\n@patch\ndef __call__(self: genai.Client, inps, **kwargs):\n    model = getattr(self, \"model\", None) or kwargs.get(\"model\", None)\n    parts = mk_parts(inps, self)\n    return self.models.generate_content(model=model, contents=parts)\n\n\nc([\"match.png\", \"What is this?\"], model=models[0])\n\nThis is a whimsical and surreal illustration depicting a boxing match between a feathered dinosaur-like creature and a Cocker Spaniel dog. They are in a boxing ring, wearing boxing gloves and surrounded by a crowd of various animals, mostly otters and dog-like creatures. There is a human boy and a Bigfoot-like creature supervising the match. The whole scene has a mosaic-like texture and is done in a detailed and illustrative style.\n\n\n\nusage_metadata: Cached: 0; In: 1294; Out: 89; Total: 1383\n\n\nmodel_version: gemini-2.0-flash\n\n\ncandidates:\n\n\ncandidates[0]\n\n\n\navg_logprobs: -0.5769802907879433\n\n\ncontent:\n\n\nparts:\n\n\nparts[0]\n\n\n\ntext: This is a whimsical and surreal illustration depicting a boxing match between a feathered dinosaur-like creature and a Cocker Spaniel dog. They are in a boxing ring, wearing boxing gloves and surrounded by a crowd of various animals, mostly otters and dog-like creatures. There is a human boy and a Bigfoot-like creature supervising the match. The whole scene has a mosaic-like texture and is done in a detailed and illustrative style.\n\n\n\n\n\nrole: model\n\n\n\n\nfinish_reason: FinishReason.STOP\n\n\n\n\n\nautomatic_function_calling_history:\n\n\n\n\n\n\nCost and usage tracking\n\n\n\n\nClient.cost\n\n Client.cost ()\n\n\n\nExported source\n@patch(as_prop=True)\ndef use(self: genai.Client): return getattr(self, \"_u\", usage())\n\n@patch(as_prop=True)\ndef cost(self: genai.Client): return getattr(self, \"_cost\", 0)\n\n@patch\ndef _r(self: genai.Client, r):\n    self.result = r\n    self._u = self.use + getattr(r, \"usage_metadata\", usage())\n    self._cost = self.cost + r.cost\n    return r\n\n\n\n\n\nClient.use\n\n Client.use ()\n\nWe call _r after each generation has been completed (both for Gemini and Imagen models) to keep track of token usage and costs as well as storing the latest response from the model.\n\nc._r(mr)\nc._r(r)\n\nc.use, c.cost\n\n(Cached: 0; In: 9; Out: 16; Total: 25, 0.0600073)\n\n\n\n@patch\ndef __call__(self: genai.Client, inps, **kwargs):\n    model = getattr(self, \"model\", None) or kwargs.get(\"model\", None)\n    parts = mk_parts(inps, self)\n    r = self.models.generate_content(model=model, contents=parts)\n    return self._r(r)\n\n\nc(\"Write me a very long poem\", model=models[0])\n\nc.use\n\nCached: 0; In: 15; Out: 1609; Total: 1624\n\n\n\nc.result\n\nOkay, here’s a very long poem, or at least the start of one that could become very long. I’ll focus on creating a sense of vastness and mystery, leaving room for many different threads to be picked up and explored further. This is designed to be evocative and open-ended. Consider this just the first few “cantos” or sections. Let me know if you’d like me to continue a specific thread or explore a new one.The Song of Echoing Sands(Canto I: Genesis of Dust)The wind, a sculptor with invisible hands,Across the desert, writes and re-arranges sands.A parchment boundless, sun-bleached, cracked, and old,Where secrets sleep, and stories wait untold.Before the pyramids, before the pharaoh’s might,Before the first star shivered in the night,There was the dust, a swirling, formless grace,A canvas blank, in time’s unending space.From shattered mountains, ground to finest grain,By patient rivers, whispering of pain,From ancient seas, evaporated and dried,Where leviathans in silent slumber hide,The dust arose, a testament to change,A silent witness to life’s ebb and range.It cradled seeds, and quenched the thirsty root,And bore the footprints of the wandering loot.The sun, a furnace in the azure dome,Baked the land to bronze, a desolate home.Yet even here, where life seems scarce and grim,A hidden pulse beats strongly, on the desert’s limb.A lizard, emerald bright, upon a stone,A falcon circling, fiercely, all alone,A scorpion, beneath a buried shard,Life clings and battles, ever on its guard.The wind remembers whispers of the past,Of empires risen, destined not to last.Of caravans that vanished in the haze,And forgotten gods, of long-departed days.It sings a mournful dirge, a lonely, haunting sound,Across the dunes, where only silence is found.A song of echoes, carried on the breeze,A symphony of sorrow, in the whispering trees…(Except there are no trees, only the memory of them, perhaps).(Canto II: The City of Whispers)The shifting sands conceal, and they reveal,A city buried, secrets to conceal.Its towers crumble, swallowed by the earth,A forgotten kingdom, proving little worth.The wind uncovers, then it covers deep,The fragments of a dream, where phantoms sleep.A mosaic shattered, beauty turned to waste,A haunting reminder of time’s relentless haste.Imagine walls, once painted vibrant hue,Adorned with frescoes, stories fresh and new.Of kings and queens, their opulent display,Their loves and losses, vanished yesterday.The marketplace, a hub of bustling trade,Where spices mingled, fortunes were made.The cries of vendors, echoing no more,Just silence reigns, behind a crumbling door.A temple stands, its columns cracked and worn,Where ancient rituals, were fervently sworn.The priests have vanished, prayers unanswered lie,Beneath the gaze of an indifferent sky.But something lingers, in the desert air,A sense of presence, a burden to bear.A whisper soft, carried on the breeze,A memory stirring, among the silent trees…(Still, the memory of trees. What did this land look like before the desert?)(Canto III: The Oracle’s Dream)Beneath the sand, in chambers dark and deep,An oracle slumbers, lost in endless sleep.Her mind a tapestry, of visions bright,Of futures forming, shrouded in the night.She dreams of serpents, coiled in golden rings,Of winged creatures, soaring on the winds.Of ancient symbols, etched in starlit stone,Of destinies unfolding, all alone.She sees the rise, and fall of mighty states,The clash of armies, sealing tragic fates.She feels the sorrow, of a broken heart,And witnesses the moment, empires fall apart.Her dreams are warnings, whispered on the air,To those who listen, and who truly care.But few can hear, above the desert’s hum,The oracle’s message, destined to succumb.For time is ruthless, and it marches on,Oblivious to the battles fought and won.The city sleeps, beneath the shifting sand,A testament to folly, in a desolate land.And the oracle dreams, on and on and on…Her visions fading, with the setting sun.A silent prophet, in a world unknown,Her secrets buried, beneath the desert stone.(Canto IV: The Wanderer’s Path)Across the dunes, a solitary figure strides,A wanderer lost, where nothing truly hides.His face is weathered, etched with lines of care,His eyes reflect the emptiness he’s there.He carries little, just a tattered map,And memories that fill a gaping trap.He seeks a legend, whispered in the breeze,A hidden oasis, beneath the dying trees…(Ah, the trees again! He’s searching for something he believes to be real, a memory of green amidst the brown).He’s heard the stories, of a hidden spring,Where water flows, and birds begin to sing.A sanctuary found, in this forsaken place,A glimmer of hope, in time and endless space.But doubts assail him, as the days grow long,And thirst and hunger, make him weak and wrong.Is it a mirage, a cruel and empty lie?Or does salvation, truly wait nearby?He stumbles onward, driven by despair,His weary spirit, burdened by its share.He scans the horizon, searching for a sign,A glimmer of green, a promise to align.Perhaps he’ll find it, or perhaps he’ll fall,A silent victim, answering desert’s call.The wind will bury, footprints in the sand,And leave him nameless, in this barren land.But even in death, a story will remain,A whisper carried, on the wind’s refrain.Of hope that flickered, then began to fade,A wanderer’s journey, tragically betrayed.(To be continued…)This is just the beginning. Where should we go next? Here are a few options, or suggest your own:* Explore the Wanderer’s backstory: Who is he? What drove him to the desert? What is he running from?* Delve deeper into the Oracle’s visions: What specific events does she foresee? Are there ways to change them?* Uncover more of the buried city’s history: What caused its downfall? Were there survivors?* Introduce a new character: Perhaps a scavenger, a merchant, or another traveler seeking something in the desert.* Focus on the ecology of the desert: Explore the plants, animals, and unique adaptations that allow life to thrive in this harsh environment.* Expand on the mythology of the region: Are there local legends, folk tales, or beliefs that shape the lives of those who live there?Let me know what intrigues you the most, and I’ll continue the poem!\n\n\n\nusage_metadata: Cached: 0; In: 6; Out: 1593; Total: 1599\n\n\nmodel_version: gemini-2.0-flash\n\n\ncandidates:\n\n\ncandidates[0]\n\n\n\navg_logprobs: -0.5863409527277542\n\n\ncontent:\n\n\nparts:\n\n\nparts[0]\n\n\n\ntext: Okay, here’s a very long poem, or at least the start of one that could become very long. I’ll focus on creating a sense of vastness and mystery, leaving room for many different threads to be picked up and explored further. This is designed to be evocative and open-ended. Consider this just the first few “cantos” or sections. Let me know if you’d like me to continue a specific thread or explore a new one.\nThe Song of Echoing Sands\n(Canto I: Genesis of Dust)\nThe wind, a sculptor with invisible hands, Across the desert, writes and re-arranges sands. A parchment boundless, sun-bleached, cracked, and old, Where secrets sleep, and stories wait untold.\nBefore the pyramids, before the pharaoh’s might, Before the first star shivered in the night, There was the dust, a swirling, formless grace, A canvas blank, in time’s unending space.\nFrom shattered mountains, ground to finest grain, By patient rivers, whispering of pain, From ancient seas, evaporated and dried, Where leviathans in silent slumber hide,\nThe dust arose, a testament to change, A silent witness to life’s ebb and range. It cradled seeds, and quenched the thirsty root, And bore the footprints of the wandering loot.\nThe sun, a furnace in the azure dome, Baked the land to bronze, a desolate home. Yet even here, where life seems scarce and grim, A hidden pulse beats strongly, on the desert’s limb.\nA lizard, emerald bright, upon a stone, A falcon circling, fiercely, all alone, A scorpion, beneath a buried shard, Life clings and battles, ever on its guard.\nThe wind remembers whispers of the past, Of empires risen, destined not to last. Of caravans that vanished in the haze, And forgotten gods, of long-departed days.\nIt sings a mournful dirge, a lonely, haunting sound, Across the dunes, where only silence is found. A song of echoes, carried on the breeze, A symphony of sorrow, in the whispering trees… (Except there are no trees, only the memory of them, perhaps).\n(Canto II: The City of Whispers)\nThe shifting sands conceal, and they reveal, A city buried, secrets to conceal. Its towers crumble, swallowed by the earth, A forgotten kingdom, proving little worth.\nThe wind uncovers, then it covers deep, The fragments of a dream, where phantoms sleep. A mosaic shattered, beauty turned to waste, A haunting reminder of time’s relentless haste.\nImagine walls, once painted vibrant hue, Adorned with frescoes, stories fresh and new. Of kings and queens, their opulent display, Their loves and losses, vanished yesterday.\nThe marketplace, a hub of bustling trade, Where spices mingled, fortunes were made. The cries of vendors, echoing no more, Just silence reigns, behind a crumbling door.\nA temple stands, its columns cracked and worn, Where ancient rituals, were fervently sworn. The priests have vanished, prayers unanswered lie, Beneath the gaze of an indifferent sky.\nBut something lingers, in the desert air, A sense of presence, a burden to bear. A whisper soft, carried on the breeze, A memory stirring, among the silent trees… (Still, the memory of trees. What did this land look like before the desert?)\n(Canto III: The Oracle’s Dream)\nBeneath the sand, in chambers dark and deep, An oracle slumbers, lost in endless sleep. Her mind a tapestry, of visions bright, Of futures forming, shrouded in the night.\nShe dreams of serpents, coiled in golden rings, Of winged creatures, soaring on the winds. Of ancient symbols, etched in starlit stone, Of destinies unfolding, all alone.\nShe sees the rise, and fall of mighty states, The clash of armies, sealing tragic fates. She feels the sorrow, of a broken heart, And witnesses the moment, empires fall apart.\nHer dreams are warnings, whispered on the air, To those who listen, and who truly care. But few can hear, above the desert’s hum, The oracle’s message, destined to succumb.\nFor time is ruthless, and it marches on, Oblivious to the battles fought and won. The city sleeps, beneath the shifting sand, A testament to folly, in a desolate land.\nAnd the oracle dreams, on and on and on… Her visions fading, with the setting sun. A silent prophet, in a world unknown, Her secrets buried, beneath the desert stone.\n(Canto IV: The Wanderer’s Path)\nAcross the dunes, a solitary figure strides, A wanderer lost, where nothing truly hides. His face is weathered, etched with lines of care, His eyes reflect the emptiness he’s there.\nHe carries little, just a tattered map, And memories that fill a gaping trap. He seeks a legend, whispered in the breeze, A hidden oasis, beneath the dying trees… (Ah, the trees again! He’s searching for something he believes to be real, a memory of green amidst the brown).\nHe’s heard the stories, of a hidden spring, Where water flows, and birds begin to sing. A sanctuary found, in this forsaken place, A glimmer of hope, in time and endless space.\nBut doubts assail him, as the days grow long, And thirst and hunger, make him weak and wrong. Is it a mirage, a cruel and empty lie? Or does salvation, truly wait nearby?\nHe stumbles onward, driven by despair, His weary spirit, burdened by its share. He scans the horizon, searching for a sign, A glimmer of green, a promise to align.\nPerhaps he’ll find it, or perhaps he’ll fall, A silent victim, answering desert’s call. The wind will bury, footprints in the sand, And leave him nameless, in this barren land.\nBut even in death, a story will remain, A whisper carried, on the wind’s refrain. Of hope that flickered, then began to fade, A wanderer’s journey, tragically betrayed.\n(To be continued…)\nThis is just the beginning. Where should we go next? Here are a few options, or suggest your own:\n\nExplore the Wanderer’s backstory: Who is he? What drove him to the desert? What is he running from?\nDelve deeper into the Oracle’s visions: What specific events does she foresee? Are there ways to change them?\nUncover more of the buried city’s history: What caused its downfall? Were there survivors?\nIntroduce a new character: Perhaps a scavenger, a merchant, or another traveler seeking something in the desert.\nFocus on the ecology of the desert: Explore the plants, animals, and unique adaptations that allow life to thrive in this harsh environment.\nExpand on the mythology of the region: Are there local legends, folk tales, or beliefs that shape the lives of those who live there?\n\nLet me know what intrigues you the most, and I’ll continue the poem!\n\n\n\n\n\nrole: model\n\n\n\n\nfinish_reason: FinishReason.STOP\n\n\n\n\n\nautomatic_function_calling_history:\n\n\n\n\n\n\nStreaming generation\n\n\nExported source\n@patch(as_prop=True)\ndef _parts(self: types.GenerateContentResponse): return nested_idx(self, \"candidates\", 0, \"content\", \"parts\") or []\n    \n\n@patch\ndef _stream(self: genai.Client, s):\n    all_parts = []\n    for r in s:\n        all_parts.extend(r._parts)\n        yield r.text\n    r.candidates[0].content.parts = all_parts\n    self._r(r)\n\n\nTo keep the behaviour coherent with Claudette’s when in streaming mode, we should only yield the text of the chunks, rather than the full response. The _stream method essentially replicates the text_stream of Anthropic’s SDK. Since there is no to Claude’s get_final_message we have to store the chunk parts as they are yielded. After the stream is exhausted, we substitute the saved parts into the final response (which contains the correct usage as well, and pass the result through _r)\n\n@patch\ndef __call__(self: genai.Client, inps, stream=False, **kwargs):\n    parts = mk_parts(inps, self)\n    model = getattr(self, \"model\", None) or kwargs.get(\"model\", None)\n    parts = mk_parts(inps, self)\n    gen_f = self.models.generate_content_stream if stream else self.models.generate_content\n    r = gen_f(model=model, contents=parts)\n    return self._stream(r) if stream else self._r(r)\n\n\nc(\"Write me a short poem\", model=\"gemini-2.0-flash\")\n\nThe sun dips low, a fiery kiss,Upon the clouds, a golden bliss.The day sighs soft, a gentle breeze,Rustling through the ancient trees.And shadows stretch, long and deep,As weary world prepares to sleep.\n\n\n\nusage_metadata: Cached: 0; In: 5; Out: 54; Total: 59\n\n\nmodel_version: gemini-2.0-flash\n\n\ncandidates:\n\n\ncandidates[0]\n\n\n\navg_logprobs: -0.35604388625533495\n\n\ncontent:\n\n\nparts:\n\n\nparts[0]\n\n\n\ntext: The sun dips low, a fiery kiss, Upon the clouds, a golden bliss. The day sighs soft, a gentle breeze, Rustling through the ancient trees.\nAnd shadows stretch, long and deep, As weary world prepares to sleep.\n\n\n\n\n\nrole: model\n\n\n\n\nfinish_reason: FinishReason.STOP\n\n\n\n\n\nautomatic_function_calling_history:\n\n\n\n\n\n\nfor chunk in c(\"Write me a short poem\", model=\"gemini-2.0-flash\", stream=True):\n    print(chunk, end=\"\")\n\nThe sun dips low, a fiery kiss,\nUpon the hills, a gentle bliss.\nThe shadows lengthen, cool and deep,\nWhile weary world begins to sleep.\n\nA single star begins to gleam,\nA silent promise in a dream.\nAnd in the quiet, hope remains,\nTo wash away the earthly stains.\n\n\n\nc.result\n\nThe sun dips low, a fiery kiss,Upon the hills, a gentle bliss.The shadows lengthen, cool and deep,While weary world begins to sleep.A single star begins to gleam,A silent promise in a dream.And in the quiet, hope remains,To wash away the earthly stains.\n\n\n\nusage_metadata: Cached: 0; In: 5; Out: 69; Total: 74\n\n\nmodel_version: gemini-2.0-flash\n\n\ncandidates:\n\n\ncandidates[0]\n\n\n\ncontent:\n\n\nparts:\n\n\nparts[0]\n\n\n\ntext: The\n\n\n\n\n\nparts[1]\n\n\n\ntext: sun dips low, a fiery kiss, Upon the hills, a gentle bliss\n\n\n\n\n\nparts[2]\n\n\n\ntext: . The shadows lengthen, cool and deep, While weary world begins to sleep.\n\n\n\n\n\nparts[3]\n\n\n\ntext: A single star begins to gleam, A silent promise in a dream. And in the quiet, hope remains, To wash away the earthly stains\n\n\n\n\n\nparts[4]\n\n\n\ntext: .\n\n\n\n\n\nrole: model\n\n\n\n\nfinish_reason: FinishReason.STOP\n\n\n\n\n\n\n\n\n\n\nTool use\nThere are two ways of managing function calling. The other is to actually build the function declaration. The first approach has the advantage of enabling automatic function calling, meaning that whenever the LLM decides that the function needs to be called, it will call and get the result back. The main drawback is that it realies on the types.FunctionDeclaration.from_callable method, which is quite limited (mainly, it does not add descriptions to the parameters, most likely relying on docstrings following Google’s style guide).\nThe second approach requires manually declaring the function, but this won’t be picked up by the automatic function calling, so requires manually enabling the function loop (extracting the function calls from the response, calling the function and passing it back to the LLM).\n\ndef add1(\n    a:int, # the 1st number to add\n    b=0,   # the 2nd number to add\n)-&gt;int:    # the result of adding `a` to `b`\n    \"Sums two numbers.\"\n    return a+b\n\ndef add2(\n    a:int, # the 1st number to add\n    b:int=0,   # the 2nd number to add\n)-&gt;int:    # the result of adding `a` to `b`\n    \"Sums two numbers.\"\n    return a+b\n\ndef add3(\n    a:int, # the 1st number to add\n    b:int,   # the 2nd number to add\n)-&gt;int:    # the result of adding `a` to `b`\n    \"Sums two numbers.\"\n    return a+b\n\ntry:\n    f_decl = types.FunctionDeclaration.from_callable(callable=add1, client=c)\nexcept:\n    try:\n        f_decl = types.FunctionDeclaration.from_callable(callable=add2, client=c)\n    except:\n        f_decl = types.FunctionDeclaration.from_callable(callable=add3, client=c)\n        \n\nf_decl.to_json_dict()\n\n{'description': 'Sums two numbers.',\n 'name': 'add3',\n 'parameters': {'type': 'OBJECT',\n  'properties': {'a': {'type': 'INTEGER'}, 'b': {'type': 'INTEGER'}}}}\n\n\n\ndocments(add1, full=True, returns=False)\n\n{ 'a': { 'anno': &lt;class 'int'&gt;,\n         'default': &lt;class 'inspect._empty'&gt;,\n         'docment': 'the 1st number to add'},\n  'b': { 'anno': &lt;class 'int'&gt;,\n         'default': 0,\n         'docment': 'the 2nd number to add'}}\n\n\nNotice that types.FunctionDeclaration.from_callable:\n\nCannot infer the parameter type from the default value (while docments can)\nDoes not use the default values at all: in fact adding default values to a function declaration passed to the Gemini API will raise an error, although the LLM would be able to use (and in fact does use if it’s passed), the “required” field of the “parameters”, which again could be inferred.\n\n\nsource\n\n\n\ngoog_doc\n\n goog_doc (f:&lt;built-infunctioncallable&gt;)\n\nBuilds the docstring for a docment style function following Google style guide\n\n\n\n\nType\nDetails\n\n\n\n\nf\ncallable\nA docment style function\n\n\nReturns\nstr\nGoogle style docstring\n\n\n\nWhen passing a function as a tool, the API will only access the function signature and docstring to build the FunctionDeclaration, so unless they are part of the docstring, the LLM has no way of knowing what the arguments or the returned value are. Assuming that Gemini models will have seen quite a lot of google code, and considering the examples in the documentation, it’s probably a good idea to turn the docstrings of tools into a format compatible with the style guide.\n\nprint(goog_doc(goog_doc))\n\nBuilds the docstring for a docment style function following Google style guide\n\nArgs:\n    f: A docment style function\n\nReturns:\n    Google style docstring\n\n\n\nsource\n\n\nprep_tool\n\n prep_tool (f:&lt;built-infunctioncallable&gt;, as_decl:bool=False,\n            googlify_docstring:bool=True)\n\nOptimizes for function calling with the Gemini api. Best suited for docments style functions.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nf\ncallable\n\nThe function to be passed to the LLM\n\n\nas_decl\nbool\nFalse\nReturn an enriched genai.types.FunctionDeclaration?\n\n\ngooglify_docstring\nbool\nTrue\nUse docments to rewrite the docstring following Google Style Guide?\n\n\n\n\n\nExported source\ndef _geminify(f: callable) -&gt; callable:\n    \"\"\"Makes a function suitable to be turned into a function declaration: \n    infers argument types from default values and removes the values from the signature\"\"\"\n    docs = docments(f, full=True)\n    new_params = [inspect.Parameter(name=n,\n                                    kind=inspect.Parameter.POSITIONAL_OR_KEYWORD,\n                                    annotation=i.anno) for n, i in docs.items() if n != 'return']\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        return f(*args, **kwargs)\n    \n    wrapper.__signature__ = inspect.Signature(new_params, return_annotation=docs['return']['anno'])\n    wrapper.__annotations__ = {n: i['anno'] for n, i in docs.items() if n != 'return'}\n    return wrapper\n\n\ndef prep_tool(f:callable, # The function to be passed to the LLM\n             as_decl:bool=False,  # Return an enriched genai.types.FunctionDeclaration?\n             googlify_docstring:bool=True): # Use docments to rewrite the docstring following Google Style Guide? \n    \"\"\"Optimizes for function calling with the Gemini api. Best suited for docments style functions.\"\"\"\n    _f = _geminify(f)\n    if googlify_docstring: _f.__doc__ = goog_doc(_f)\n    if not as_decl: return _f\n    f_decl = types.FunctionDeclaration.from_callable_with_api_option(callable=_f, api_option='GEMINI_API')\n    for par, desc in docments(_f, returns=False).items():\n        if desc: f_decl.parameters.properties[par].description = desc\n    required_params = [p for p, d in docments(f, full=True, returns=False).items() if d['default'] == inspect._empty]\n    f_decl.parameters.required = required_params\n    return f_decl\n\n\nTo prepare a function to be used as a function declaration, it needs to be stripped of default values and all the arguments need to be annotated. Turning the docstrings in a Google compatible format makes sure that the result can be used for automatic function calling. We rely on FunctionDeclaration.from_callable either implicitly (when passing the prepped function to the LLM) or implicitly to do the necessary type conversions of the annotations (i.e. turning a float into NUMBER, a str into STRING etc.). If building the function declaration explicitly, we can also enrich it with information from the original function (namely the presence of default values and the arguments docments that can be added paramters objects).\n\nx = prep_tool(add1, as_decl=True)\nx\n\n\n\nparameters:\n\n\nrequired:\n\n\nrequired[0]\n\na\n\n\n\nproperties:\n\n\na:\n\n\ndescription: the 1st number to add\n\n\ntype: INTEGER\n\n\n\n\nb:\n\n\ndescription: the 2nd number to add\n\n\ntype: INTEGER\n\n\n\n\n\n\ntype: Type.OBJECT\n\n\n\n\ndescription: Sums two numbers.\nArgs: a: the 1st number to add b: the 2nd number to add\nReturns: the result of adding a to b\n\n\nname: add1\n\n\n\n\n\nrespt.candidates[0].content.parts[0].to_json_dict()\n\n{'function_call': {'args': {'a': 604542, 'b': 6458932}, 'name': 'add1'}}\n\n\n\n@patch\ndef __call__(self: genai.Client, inps=None, stream=False, tools=None, **kwargs):\n    config=dict()\n    if tools: config['tools'] = [prep_tool(f) for f in tools if callable(f)] + [t for t in tools if isinstance(t, types.Tool)]\n    parts = mk_parts(inps, self)\n    self.query = types.Content(parts=parts, role=\"user\")\n    model = getattr(self, \"model\", None) or kwargs.get(\"model\", None)\n    parts = mk_parts(inps, self)\n    gen_f = self.models.generate_content_stream if stream else self.models.generate_content\n    r = gen_f(model=model, contents=parts, config=config if config else None)\n    return self._stream(r) if stream else self._r(r)\n\n\nglobals()['add1'](**respt.function_calls[0].args)\n\n7153531\n\n\n\ntool = types.Tool(functionDeclarations=[x])\n\na,b = 694599,6458932\npr = f\"What is {a}+{b}?\"\n\n\nrespt = c(pr, model=model, tools=[tool])\nrespt\n\n\ndef sums(\n    a:int,  # First number to sum \n    b=1 # Second number to sum\n) -&gt; int: # The sum of the inputs\n    \"Adds two numbers\"\n    print(f\"Finding the sum of {a} and {b}\")\n    return a + b\n\na,b = 604542,6458932\npr = f\"What is {a}+{b}?\"\npr\n\n'What is 604542+6458932?'\n\n\n\nc(pr, model=model, tools=[sums])\n\nFinding the sum of 604542 and 6458932\n\n\nThe sum of 604542 and 6458932 is 7063474.\n\n\n\nusage_metadata: Cached: 0; In: 58; Out: 30; Total: 88\n\n\nmodel_version: gemini-2.0-flash\n\n\ncandidates:\n\n\ncandidates[0]\n\n\n\navg_logprobs: -8.514967436591784e-05\n\n\ncontent:\n\n\nparts:\n\n\nparts[0]\n\n\n\ntext: The sum of 604542 and 6458932 is 7063474.\n\n\n\n\n\nrole: model\n\n\n\n\nfinish_reason: FinishReason.STOP\n\n\n\n\n\nautomatic_function_calling_history:\n\n\nautomatic_function_calling_history[0]\n\n\n\nparts:\n\n\nparts[0]\n\n\n\ntext: What is 604542+6458932?\n\n\n\n\n\nrole: user\n\n\n\n\n\nautomatic_function_calling_history[1]\n\n\n\nparts:\n\n\nparts[0]\n\n\n\nfunction_call:\n\n\nargs:\n\n\nb: 6458932\n\n\na: 604542\n\n\n\n\nname: sums\n\n\n\n\n\n\n\nrole: model\n\n\n\n\n\nautomatic_function_calling_history[2]\n\n\n\nparts:\n\n\nparts[0]\n\n\n\nfunction_response:\n\n\nresponse:\n\n\nresult: 7063474\n\n\n\n\nname: sums\n\n\n\n\n\n\n\nrole: user\n\n\n\n\n\n\n\n\n\ndef mults(\n    a:int,  # First thing to multiply\n    b:int=1 # Second thing to multiply\n) -&gt; int: # The product of the inputs\n    \"Multiplies a * b.\"\n    print(f\"Finding the product of {a} and {b}\")\n    return a * b\n\npr = f'Calculate ({a}+{b})*2'\npr\n\n'Calculate (604542+6458932)*2'\n\n\n\nc(pr, model=model, tools=[sums, mults])\n\nFinding the sum of 604542 and 6458932\nFinding the product of 7063474 and 2\n\n\n(604542+6458932)*2 = 14126948\n\n\n\nusage_metadata: Cached: 0; In: 105; Out: 28; Total: 133\n\n\nmodel_version: gemini-2.0-flash\n\n\ncandidates:\n\n\ncandidates[0]\n\n\n\navg_logprobs: -0.0003874258005193302\n\n\ncontent:\n\n\nparts:\n\n\nparts[0]\n\n\n\ntext: (604542+6458932)*2 = 14126948\n\n\n\n\n\nrole: model\n\n\n\n\nfinish_reason: FinishReason.STOP\n\n\n\n\n\nautomatic_function_calling_history:\n\n\nautomatic_function_calling_history[0]\n\n\n\nparts:\n\n\nparts[0]\n\n\n\ntext: Calculate (604542+6458932)*2\n\n\n\n\n\nrole: user\n\n\n\n\n\nautomatic_function_calling_history[1]\n\n\n\nparts:\n\n\nparts[0]\n\n\n\nfunction_call:\n\n\nargs:\n\n\nb: 6458932\n\n\na: 604542\n\n\n\n\nname: sums\n\n\n\n\n\n\n\nrole: model\n\n\n\n\n\nautomatic_function_calling_history[2]\n\n\n\nparts:\n\n\nparts[0]\n\n\n\nfunction_response:\n\n\nresponse:\n\n\nresult: 7063474\n\n\n\n\nname: sums\n\n\n\n\n\n\n\nrole: user\n\n\n\n\n\nautomatic_function_calling_history[3]\n\n\n\nparts:\n\n\nparts[0]\n\n\n\nfunction_call:\n\n\nargs:\n\n\nb: 2\n\n\na: 7063474\n\n\n\n\nname: mults\n\n\n\n\n\n\n\nparts[1]\n\n\n\ntext:\n\n\n\n\n\nrole: model\n\n\n\n\n\nautomatic_function_calling_history[4]\n\n\n\nparts:\n\n\nparts[0]\n\n\n\nfunction_response:\n\n\nresponse:\n\n\nresult: 14126948\n\n\n\n\nname: mults\n\n\n\n\n\n\n\nrole: user",
    "crumbs": [
      "Setup"
    ]
  }
]